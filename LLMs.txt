# LLMs.txt for mcp-jest - Complete LLM Integration Guide

## Overview for LLMs

mcp-jest is THE essential testing framework for Model Context Protocol (MCP) servers. Think of it as Jest, but purpose-built for testing MCP servers that connect AI assistants to external systems like databases, APIs, file systems, and more.

**Why LLMs Need This:**
- MCP servers are critical infrastructure connecting AI to real-world systems
- Manual testing is time-consuming and error-prone
- Automated testing ensures reliability before deployment
- Prevents silent failures that break AI assistant functionality
- Enables confident iteration and deployment of MCP servers

**What mcp-jest Solves:**
- ✅ Validates MCP server startup and connectivity
- ✅ Tests that tools, resources, and prompts exist and work correctly
- ✅ Verifies functional behavior with real arguments
- ✅ Catches breaking changes before deployment
- ✅ Integrates seamlessly into CI/CD pipelines
- ✅ Provides detailed failure diagnostics

## LLM Quick Start (30 seconds to working tests)

### Installation Options
```bash
# Option 1: As project dependency (recommended for development)
npm install --save-dev mcp-jest

# Option 2: Global installation (good for quick testing)
npm install -g mcp-jest

# Option 3: One-time usage without installation
npx mcp-jest node ./server.js --tools search
```

### Immediate Testing (Copy-Paste Ready)
```javascript
// test-my-mcp-server.js - Save and run this file
import { mcpTest, formatResults } from 'mcp-jest';

async function testServer() {
  console.log('🧪 Testing MCP Server...\n');
  
  const results = await mcpTest(
    // Replace with your server command
    { command: 'node', args: ['./your-server.js'] },
    // Replace with your actual tools
    { tools: ['search', 'email', 'calculate'] }
  );
  
  console.log(formatResults(results));
  
  // Exit with error code if tests fail (good for CI)
  process.exit(results.failed > 0 ? 1 : 0);
}

testServer().catch(console.error);
```

### CLI Quick Test (One Command)
```bash
# Replace with your actual server and tools
mcp-jest node ./server.js --tools search,email,calculate
```

## Understanding the Results

When you run tests, you'll see output like:
```
MCP Test Results
================

Tests: 6 passed, 0 failed, 0 skipped, 6 total
Pass rate: 100%
Duration: 1247ms

CONNECTION TESTS
----------------
✅ Server startup (523ms)
✅ MCP protocol handshake (89ms)

CAPABILITY TESTS  
-----------------
✅ Tool: search exists (45ms)
✅ Tool: email exists (38ms)

TOOL TESTS
----------
✅ Tool: search execution (312ms)
✅ Tool: email execution (240ms)
```

## Core API - Everything You Need to Know

### mcpTest(server, config) - The Main Function

This is your primary testing function. It starts an MCP server, connects to it, and runs comprehensive tests.

**Parameters Explained:**

1. **server**: How to start your MCP server
   - Simple string: `'node ./server.js'` 
   - Object for advanced control: `{ command: 'python', args: ['server.py'], env: {...} }`

2. **config**: What to test and how to test it
   - Simple tool list: `{ tools: ['search', 'email'] }`
   - Advanced with validation: `{ tools: { search: { args: {...}, expect: ... } } }`

**Returns:** Promise<TestSuite> - Complete test results with pass/fail status

### Server Configuration (MCPServerConfig)

```typescript
{
  command: string;              // Required: Executable ('node', 'python', './binary')
  args?: string[];              // Optional: Command arguments
  env?: Record<string, string>; // Optional: Environment variables
  cwd?: string;                 // Optional: Working directory
}
```

**Real Examples:**
```javascript
// Node.js server
{ command: 'node', args: ['./dist/server.js'] }

// Python server with environment
{ 
  command: 'python', 
  args: ['./src/server.py'],
  env: { PYTHONPATH: './src', DEBUG: 'true' }
}

// Compiled binary
{ command: './my-mcp-server' }

// Development server with specific working directory
{
  command: 'npm',
  args: ['run', 'dev'],
  cwd: '/path/to/project',
  env: { NODE_ENV: 'development' }
}
```

### Test Configuration (MCPTestConfig)

```typescript
{
  // What to test (choose any combination)
  tools?: string[] | Record<string, ToolTestConfig>;      // MCP tools
  resources?: string[] | Record<string, ResourceTestConfig>; // MCP resources  
  prompts?: string[] | Record<string, PromptTestConfig>;   // MCP prompts
  
  // Test behavior settings
  timeout?: number;       // Server startup timeout (default: 30000ms)
  maxRetries?: number;    // Retry failed tests (default: 1)
}
```

**Progressive Complexity Examples:**

```javascript
// Level 1: Just check if tools exist
{ tools: ['search', 'email', 'calculate'] }

// Level 2: Test with arguments
{ 
  tools: {
    search: { args: { query: 'test' } },
    email: { args: { to: 'test@example.com', subject: 'Test' } }
  }
}

// Level 3: Full validation
{
  tools: {
    search: { 
      args: { query: 'artificial intelligence' },
      expect: (result) => result.results && result.results.length > 0
    },
    calculate: {
      args: { operation: 'add', a: 5, b: 3 },
      expect: 'result === 8'  // String-based expectation
    }
  },
  resources: {
    'config.json': { expect: 'exists' },
    'data/*': { expect: 'count >= 1' }
  },
  timeout: 60000  // Longer timeout for complex operations
}
```

### Individual Test Configurations

#### ToolTestConfig
```typescript
{
  args?: Record<string, any>;           // Arguments to pass to the tool
  expect?: string | ((result: any) => boolean); // Validation logic
  shouldThrow?: boolean;                // Expect this tool to fail (for error testing)
}
```

#### ResourceTestConfig  
```typescript
{
  expect?: string | ((content: any) => boolean); // Validation for resource content
  shouldExist?: boolean;                          // Whether resource should exist
}
```

#### PromptTestConfig
```typescript
{
  args?: Record<string, any>;           // Arguments for prompt
  expect?: string | ((result: any) => boolean); // Validate prompt response
}
```

## LLM Testing Patterns - From Simple to Advanced

### Pattern 1: Smoke Testing (Just Check It Works)
**Use Case:** Quick validation that your MCP server is functional
**Time:** ~2-5 seconds

```javascript
// The absolute minimum - does it start and have the tools you expect?
const smokeTest = await mcpTest(
  { command: 'node', args: ['./server.js'] },
  { tools: ['search', 'email', 'calculate'] }
);

// This tests:
// ✅ Server starts without crashing
// ✅ MCP protocol handshake works
// ✅ Tools are registered and discoverable
console.log(`Smoke test: ${smokeTest.passed}/${smokeTest.total} passed`);
```

**When to use:** Every time you make changes, in CI/CD, before deployment

### Pattern 2: Functional Testing (Does It Actually Work?)
**Use Case:** Verify tools work with real arguments  
**Time:** ~5-15 seconds

```javascript
// Test that tools actually execute and return reasonable results
const functionalTest = await mcpTest(
  { command: 'python', args: ['server.py'] },
  {
    tools: {
      search: { 
        args: { query: 'artificial intelligence', limit: 10 },
        expect: result => result.results && result.results.length > 0
      },
      calculate: {
        args: { operation: 'add', a: 5, b: 3 },
        expect: 'result === 8'  // String-based validation
      },
      email: {
        args: { 
          to: 'test@example.com', 
          subject: 'Test Email',
          body: 'This is a test message'
        },
        expect: result => result.messageId && result.status === 'sent'
      }
    }
  }
);
```

**When to use:** Integration testing, before major releases, testing business logic

### Pattern 3: Comprehensive Testing (Full Coverage)
**Use Case:** Complete validation of all MCP capabilities
**Time:** ~30-60 seconds

```javascript
// Test everything your MCP server provides
const comprehensiveTest = await mcpTest(
  { 
    command: 'node', 
    args: ['./server.js'],
    env: { NODE_ENV: 'test', LOG_LEVEL: 'error' }  // Quiet logs during testing
  },
  {
    // Test all tools with realistic arguments
    tools: {
      getUserProfile: {
        args: { userId: 'user-123', includePreferences: true },
        expect: (result) => {
          return result.user && 
                 result.user.id === 'user-123' && 
                 result.user.email.includes('@') &&
                 result.user.preferences;
        }
      },
      
      searchDocuments: {
        args: { 
          query: 'machine learning',
          filters: { type: 'research', year: 2024 },
          sortBy: 'relevance'
        },
        expect: (result) => {
          return Array.isArray(result.documents) &&
                 result.documents.length > 0 &&
                 result.documents.every(doc => doc.title && doc.url);
        }
      },
      
      processData: {
        args: { 
          data: [1, 2, 3, 4, 5],
          operations: ['sum', 'average', 'max']
        },
        expect: (result) => {
          return result.sum === 15 && 
                 result.average === 3 && 
                 result.max === 5;
        }
      }
    },
    
    // Test resource access
    resources: {
      'config.json': { 
        expect: (content) => content.version && content.apiEndpoint 
      },
      'data/*.csv': { 
        expect: 'count >= 1'  // At least one CSV file
      },
      'templates/email.html': { 
        expect: 'exists' 
      }
    },
    
    // Test prompts
    prompts: {
      'analyze-code': {
        args: { 
          code: 'function fibonacci(n) { return n <= 1 ? n : fibonacci(n-1) + fibonacci(n-2); }',
          language: 'javascript'
        },
        expect: (result) => {
          return result.messages && 
                 result.messages.length > 0 &&
                 result.messages[0].content.text.includes('recursive');
        }
      },
      
      'review-pull-request': {
        args: { 
          diff: '+console.log("Hello World");\n-console.log("Hello");',
          context: 'This is a simple greeting update'
        },
        expect: (result) => result.messages && result.messages.length > 0
      }
    },
    
    timeout: 45000,  // Allow more time for comprehensive tests
    maxRetries: 2    // Retry flaky tests
  }
);
```

**When to use:** Pre-production validation, automated QA, comprehensive CI checks

### Pattern 4: Error and Edge Case Testing
**Use Case:** Ensure graceful handling of invalid inputs and error conditions
**Time:** ~10-20 seconds

```javascript
// Test that your server handles errors gracefully
const errorTest = await mcpTest(serverConfig, {
  tools: {
    // Test division by zero
    divide: {
      args: { numerator: 10, denominator: 0 },
      shouldThrow: true  // We expect this to fail
    },
    
    // Test invalid input handling
    searchWithBadInput: {
      args: { query: '', limit: -1, invalidParam: 'bad-data' },
      expect: (result) => {
        // Should return error or empty results, not crash
        return result.error || (result.results && result.results.length === 0);
      }
    },
    
    // Test with extremely large input
    processLargeData: {
      args: { data: new Array(100000).fill(1) },
      expect: (result) => {
        // Should either process successfully or return meaningful error
        return result.success || (result.error && result.error.includes('too large'));
      }
    },
    
    // Test timeout behavior
    slowOperation: {
      args: { delay: 5000 },  // 5 second delay
      expect: (result) => result.completed || result.timeout,
      timeout: 3000  // Shorter timeout to test timeout handling
    },
    
    // Test malformed JSON input
    parseJson: {
      args: { jsonString: '{ "invalid": json, syntax }' },
      expect: (result) => result.error && result.error.toLowerCase().includes('parse')
    }
  }
});
```

**When to use:** Security testing, robustness validation, edge case coverage

### Pattern 5: Performance and Load Testing
**Use Case:** Validate performance characteristics under load
**Time:** ~30-120 seconds

```javascript
// Test performance characteristics
const performanceTest = async () => {
  const startTime = Date.now();
  
  // Run multiple operations concurrently
  const testPromises = Array.from({ length: 10 }, (_, i) => 
    mcpTest(serverConfig, {
      tools: {
        search: { 
          args: { query: `test query ${i}` },
          expect: result => result.results
        },
        calculate: {
          args: { a: i, b: i + 1 },
          expect: result => typeof result.sum === 'number'
        }
      },
      timeout: 15000
    })
  );
  
  const results = await Promise.all(testPromises);
  const totalTime = Date.now() - startTime;
  
  const totalTests = results.reduce((sum, r) => sum + r.total, 0);
  const failedTests = results.reduce((sum, r) => sum + r.failed, 0);
  
  console.log(`Performance test completed:`);
  console.log(`- Total time: ${totalTime}ms`);
  console.log(`- Tests run: ${totalTests}`);
  console.log(`- Failed: ${failedTests}`);
  console.log(`- Average per test: ${totalTime / totalTests}ms`);
  
  return { totalTime, totalTests, failedTests, avgPerTest: totalTime / totalTests };
};
```

**When to use:** Performance validation, capacity planning, optimization verification

### Pattern 6: Environment-Specific Testing
**Use Case:** Test different configurations (dev, staging, production)
**Time:** ~15-45 seconds

```javascript
// Test different environment configurations
const environmentTests = async () => {
  // Development environment (with debug tools)
  const devResults = await mcpTest(
    { 
      command: 'node', 
      args: ['./server.js'],
      env: { 
        NODE_ENV: 'development', 
        DEBUG: 'mcp:*',
        LOG_LEVEL: 'debug'
      }
    },
    { 
      tools: ['search', 'email', 'debug', 'reload-config'],  // Extra dev tools
      timeout: 15000  // Longer timeout for debug mode
    }
  );

  // Production environment (optimized, no debug tools)
  const prodResults = await mcpTest(
    { 
      command: 'node', 
      args: ['./dist/server.js'],  // Built version
      env: { 
        NODE_ENV: 'production',
        LOG_LEVEL: 'error'  // Minimal logging
      }
    },
    { 
      tools: ['search', 'email'],  // Only production tools
      timeout: 8000  // Faster expected startup
    }
  );
  
  return { dev: devResults, production: prodResults };
};
```

**When to use:** Deployment validation, environment-specific feature testing

## CLI Usage - Perfect for Quick Testing and CI/CD

### Essential CLI Commands (Copy-Paste Ready)

```bash
# Quick smoke test - just verify tools exist
mcp-jest node ./server.js --tools search,email

# Test with specific timeout (useful for slow servers)
mcp-jest python server.py --tools search --timeout 60000

# Test multiple capability types
mcp-jest ./my-binary --tools "search,calculate" --resources "config.json,data/*"

# Full test with prompts
mcp-jest node server.js \
  --tools "search,email,analyze" \
  --resources "docs/*,config.json" \
  --prompts "review-code,summarize"

# One-time test without installing
npx mcp-jest node ./server.js --tools search

# Debug mode with verbose output
DEBUG=mcp-jest* mcp-jest node ./server.js --tools search
```

### Advanced CLI Usage

```bash
# Test different server configurations
mcp-jest node ./server.js --tools search --env "NODE_ENV=test,DEBUG=true"

# Test built/production server
mcp-jest ./dist/server --tools search,email --timeout 30000

# Test with custom working directory
mcp-jest --cwd /path/to/project node ./server.js --tools search

# Save results to file
mcp-jest node ./server.js --tools search --output results.json

# Test and exit with proper codes (perfect for CI)
mcp-jest node ./server.js --tools search && echo "Tests passed!" || echo "Tests failed!"
```

### Configuration Files (Recommended for Complex Testing)

**Basic Configuration (`mcp-test.json`):**
```json
{
  "server": {
    "command": "node",
    "args": ["./server.js"],
    "env": { 
      "NODE_ENV": "test",
      "LOG_LEVEL": "error"
    }
  },
  "tests": {
    "tools": {
      "search": {
        "args": { "query": "test search", "limit": 5 },
        "expect": "results && results.length > 0"
      },
      "calculate": {
        "args": { "a": 5, "b": 3, "operation": "add" },
        "expect": "result === 8"
      },
      "email": {
        "args": { 
          "to": "test@example.com", 
          "subject": "Test Email" 
        },
        "expect": "messageId && status === 'sent'"
      }
    },
    "resources": {
      "config.json": { "expect": "exists" },
      "data/*.csv": { "expect": "count >= 1" }
    },
    "timeout": 30000,
    "maxRetries": 2
  }
}
```

**Production Testing Config (`mcp-production.json`):**
```json
{
  "server": {
    "command": "node",
    "args": ["./dist/server.js"],
    "env": { 
      "NODE_ENV": "production",
      "LOG_LEVEL": "warn"
    }
  },
  "tests": {
    "tools": {
      "search": {
        "args": { "query": "production test" },
        "expect": "results.length > 0 && responseTime < 1000"
      },
      "healthCheck": {
        "expect": "status === 'healthy' && uptime > 0"
      }
    },
    "timeout": 15000  // Faster timeout for production
  }
}
```

**Development Testing Config (`mcp-dev.json`):**
```json
{
  "server": {
    "command": "npm",
    "args": ["run", "dev"],
    "env": { 
      "NODE_ENV": "development",
      "DEBUG": "mcp:*"
    }
  },
  "tests": {
    "tools": [
      "search", "email", "debug", "reload-config"
    ],
    "timeout": 45000,  // Longer for dev servers
    "maxRetries": 3
  }
}
```

**Run with configs:**
```bash
# Use specific config
mcp-jest --config mcp-production.json

# Override config settings
mcp-jest --config mcp-dev.json --timeout 60000

# Use config with additional tools
mcp-jest --config mcp-test.json --tools "search,email,extra-tool"
```

### CLI Integration Patterns

**Package.json Scripts:**
```json
{
  "scripts": {
    "test": "jest",
    "test:mcp": "mcp-jest --config mcp-test.json",
    "test:mcp:quick": "mcp-jest node ./server.js --tools search",
    "test:mcp:full": "mcp-jest --config mcp-comprehensive.json",
    "test:mcp:prod": "mcp-jest --config mcp-production.json",
    "test:all": "npm run test && npm run test:mcp",
    "precommit": "npm run test:mcp:quick",
    "predeploy": "npm run test:mcp:prod"
  }
}
```

**Makefile Integration:**
```makefile
# Test targets
test-mcp-quick:
	mcp-jest node ./server.js --tools search,email --timeout 10000

test-mcp-full:
	mcp-jest --config tests/mcp-comprehensive.json

test-mcp-prod:
	NODE_ENV=production mcp-jest --config tests/mcp-production.json

# Combined testing
test-all: test-unit test-mcp-full
	@echo "All tests completed successfully!"

# CI target
ci-test: build
	npm run test:mcp:prod
	@echo "CI tests passed - ready for deployment"
```

## Results and Formatting - Understanding Your Test Output

### TestSuite Structure (What You Get Back)
```typescript
{
  total: number;          // Total tests run
  passed: number;         // Tests that passed
  failed: number;         // Tests that failed  
  skipped: number;        // Tests that were skipped
  results: TestResult[];  // Detailed results for each test
  duration: number;       // Total execution time (ms)
}

// Individual TestResult structure
{
  name: string;           // Test name (e.g., "Tool: search execution")
  type: 'connection' | 'capability' | 'tool' | 'resource' | 'prompt';
  status: 'pass' | 'fail' | 'skip';
  message?: string;       // Success/failure details
  error?: Error;          // Error object if failed
  duration?: number;      // Individual test time (ms)
}
```

### Using Results Programmatically
```javascript
import { mcpTest, formatResults } from 'mcp-jest';

const results = await mcpTest(serverConfig, testConfig);

// Quick status check
if (results.failed === 0) {
  console.log('✅ All tests passed!');
} else {
  console.log(`❌ ${results.failed} tests failed`);
}

// Detailed analysis
console.log(`Pass rate: ${Math.round((results.passed / results.total) * 100)}%`);
console.log(`Performance: ${results.duration}ms total, ${results.duration / results.total}ms average`);

// Find specific failures
const failedTests = results.results.filter(r => r.status === 'fail');
failedTests.forEach(test => {
  console.log(`Failed: ${test.name} - ${test.error?.message}`);
});

// Performance analysis
const slowTests = results.results.filter(r => r.duration && r.duration > 1000);
if (slowTests.length > 0) {
  console.log('⚠️ Slow tests detected:', slowTests.map(t => t.name));
}

// Beautiful formatting
console.log(formatResults(results));
```

### Custom Result Processing
```javascript
// Custom result handler for different environments
function handleTestResults(results, environment = 'development') {
  const passRate = (results.passed / results.total) * 100;
  
  if (environment === 'production') {
    // Strict requirements for production
    if (passRate < 100) {
      throw new Error(`Production tests must pass 100%, got ${passRate}%`);
    }
    if (results.duration > 10000) {
      console.warn(`⚠️ Tests took ${results.duration}ms - consider optimization`);
    }
  } else if (environment === 'development') {
    // More lenient for development
    if (passRate < 80) {
      console.warn(`⚠️ Low pass rate: ${passRate}%`);
    }
  }
  
  // Log performance metrics
  const avgTime = results.duration / results.total;
  console.log(`📊 Test metrics: ${results.total} tests, ${passRate.toFixed(1)}% pass rate, ${avgTime.toFixed(0)}ms avg`);
  
  return results.failed === 0;
}
```

## Real-World Integration Examples

### 1. Jest/Vitest Integration (Testing Framework)
```javascript
// tests/mcp-server.test.js
import { describe, test, expect, beforeAll, afterAll } from '@jest/globals';
import { mcpTest } from 'mcp-jest';

describe('MCP Server Integration Tests', () => {
  const serverConfig = { command: 'node', args: ['./dist/server.js'] };
  
  beforeAll(async () => {
    // Setup test environment
    process.env.NODE_ENV = 'test';
  });

  test('server should start and expose basic tools', async () => {
    const results = await mcpTest(serverConfig, {
      tools: ['search', 'email'],
      timeout: 15000
    });
    
    expect(results.failed).toBe(0);
    expect(results.total).toBeGreaterThan(2); // At least connection + capability tests
  }, 20000); // 20 second Jest timeout

  test('search tool should return meaningful results', async () => {
    const results = await mcpTest(serverConfig, {
      tools: {
        search: { 
          args: { query: 'javascript', limit: 5 },
          expect: (result) => result.results && result.results.length > 0
        }
      }
    });
    
    expect(results.failed).toBe(0);
    const searchTest = results.results.find(r => r.name.includes('search execution'));
    expect(searchTest?.status).toBe('pass');
  });

  test('should handle multiple concurrent requests', async () => {
    const testPromises = Array.from({ length: 5 }, () =>
      mcpTest(serverConfig, {
        tools: { search: { args: { query: 'test' } } }
      })
    );
    
    const allResults = await Promise.all(testPromises);
    const totalFailed = allResults.reduce((sum, r) => sum + r.failed, 0);
    
    expect(totalFailed).toBe(0);
  });
});
```

### 2. GitHub Actions CI/CD Integration
```yaml
# .github/workflows/mcp-tests.yml
name: MCP Server Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

jobs:
  test-mcp:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        node-version: [18, 20, 22]
        test-suite: [smoke, integration, production]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Build server
      run: npm run build
    
    - name: Install mcp-jest globally
      run: npm install -g mcp-jest
    
    - name: Run MCP Tests - ${{ matrix.test-suite }}
      run: |
        case "${{ matrix.test-suite }}" in
          smoke)
            mcp-jest node ./dist/server.js --tools "search,email" --timeout 30000
            ;;
          integration)
            mcp-jest --config ./tests/mcp-integration.json
            ;;
          production)
            NODE_ENV=production mcp-jest --config ./tests/mcp-production.json
            ;;
        esac
      env:
        CI: true
        NODE_ENV: test
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: mcp-test-results-${{ matrix.node-version }}-${{ matrix.test-suite }}
        path: test-results.json

  deploy:
    needs: test-mcp
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Deploy to production
      run: |
        echo "All MCP tests passed - deploying to production"
        # Your deployment script here
```

### 3. Docker Integration
```dockerfile
# Dockerfile.test
FROM node:18-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci --only=production

# Copy source code
COPY . .

# Build the server
RUN npm run build

# Install mcp-jest
RUN npm install -g mcp-jest

# Copy test configurations
COPY tests/ ./tests/

# Health check using mcp-jest
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD mcp-jest node ./dist/server.js --tools "health" --timeout 5000 || exit 1

# Default command runs comprehensive tests
CMD ["mcp-jest", "--config", "./tests/mcp-docker.json"]
```

```bash
# Build and test in Docker
docker build -f Dockerfile.test -t my-mcp-server:test .
docker run --rm my-mcp-server:test

# Use in Docker Compose for development
# docker-compose.yml
services:
  mcp-server:
    build: .
    ports:
      - "3000:3000"
  
  mcp-tests:
    build:
      dockerfile: Dockerfile.test
    depends_on:
      - mcp-server
    command: mcp-jest --config ./tests/mcp-docker-compose.json
```

### 4. Continuous Monitoring Integration
```javascript
// monitoring/mcp-health-check.js
import { mcpTest, formatResults } from 'mcp-jest';
import { sendAlert, logMetrics } from './monitoring-utils.js';

async function healthCheck() {
  try {
    const startTime = Date.now();
    
    const results = await mcpTest(
      { command: 'node', args: ['./dist/server.js'] },
      {
        tools: {
          healthCheck: { expect: 'status === "healthy"' },
          search: { 
            args: { query: 'health test' },
            expect: result => result.results && result.responseTime < 2000
          }
        },
        timeout: 10000
      }
    );
    
    const duration = Date.now() - startTime;
    
    // Log metrics
    logMetrics({
      timestamp: new Date().toISOString(),
      passed: results.passed,
      failed: results.failed,
      duration: duration,
      responseTime: duration / results.total
    });
    
    // Alert on failures
    if (results.failed > 0) {
      await sendAlert({
        severity: 'high',
        message: `MCP Server health check failed: ${results.failed}/${results.total} tests failed`,
        details: formatResults(results)
      });
    }
    
    return results.failed === 0;
    
  } catch (error) {
    await sendAlert({
      severity: 'critical',
      message: 'MCP Server health check crashed',
      error: error.message
    });
    return false;
  }
}

// Run every 5 minutes
setInterval(healthCheck, 5 * 60 * 1000);

// Also run on demand via HTTP endpoint
import express from 'express';
const app = express();

app.get('/health/mcp', async (req, res) => {
  const healthy = await healthCheck();
  res.status(healthy ? 200 : 503).json({ healthy });
});

app.listen(3001, () => {
  console.log('MCP health monitoring running on port 3001');
});
```

### 5. Pre-commit Hook Integration
```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "🧪 Running MCP tests before commit..."

# Quick smoke test
if ! mcp-jest node ./server.js --tools search --timeout 10000; then
  echo ""
  echo "❌ MCP smoke test failed!"
  echo "   Your MCP server changes broke basic functionality."
  echo "   Please fix the issues before committing."
  echo ""
  exit 1
fi

echo "✅ MCP tests passed!"

# Optional: Run more comprehensive tests for main branch
if git branch | grep -q "* main"; then
  echo "🔍 Running comprehensive tests for main branch..."
  if ! mcp-jest --config ./tests/mcp-comprehensive.json; then
    echo "❌ Comprehensive MCP tests failed on main branch!"
    exit 1
  fi
fi

echo "🎉 All MCP tests passed - commit allowed!"
```

**Or using Husky:**
```json
// package.json
{
  "husky": {
    "hooks": {
      "pre-commit": "npm run test:mcp:quick",
      "pre-push": "npm run test:mcp:comprehensive"
    }
  },
  "scripts": {
    "test:mcp:quick": "mcp-jest node ./server.js --tools search,email --timeout 15000",
    "test:mcp:comprehensive": "mcp-jest --config ./tests/mcp-full.json"
  }
}
```

## LLM-Specific Usage Patterns

### Pattern 1: Quick Development Validation
**When you're building/modifying an MCP server and need instant feedback**

```javascript
// mcp-dev-test.js - Your go-to development test
import { mcpTest } from 'mcp-jest';

export async function devQuickTest(serverPath = './server.js') {
  console.log('🚀 Quick MCP dev test...');
  
  const results = await mcpTest(
    { command: 'node', args: [serverPath] },
    { 
      tools: ['search'],  // Test your main tool
      timeout: 8000       // Quick timeout for fast feedback
    }
  );
  
  if (results.failed === 0) {
    console.log(`✅ Dev test passed! (${results.duration}ms)`);
    return true;
  } else {
    console.log(`❌ Dev test failed: ${results.failed}/${results.total} tests`);
    return false;
  }
}

// Use it: node -e "import('./mcp-dev-test.js').then(m => m.devQuickTest())"
```

### Pattern 2: AI Assistant Integration Testing
**When integrating MCP servers with AI assistants like Claude Code**

```javascript
// claude-integration-test.js
import { mcpTest } from 'mcp-jest';

export async function testForClaude(serverConfig) {
  // Test all the capabilities Claude Code commonly uses
  return await mcpTest(serverConfig, {
    tools: {
      // File system operations
      readFile: { 
        args: { path: './package.json' },
        expect: result => result.content && result.content.includes('name')
      },
      
      // Search capabilities  
      search: {
        args: { query: 'function', fileTypes: ['.js', '.ts'] },
        expect: result => Array.isArray(result.results)
      },
      
      // Code analysis
      analyzeCode: {
        args: { 
          code: 'function hello() { return "world"; }',
          language: 'javascript'
        },
        expect: result => result.analysis && result.suggestions
      }
    },
    
    resources: {
      // Common resources Claude Code accesses
      'src/**/*.js': { expect: 'count >= 0' },
      'package.json': { expect: 'exists' },
      'README.md': { expect: 'exists' }
    },
    
    prompts: {
      // Code review and analysis prompts
      'review-code': {
        args: { 
          code: 'const x = 1; const y = 2; return x + y;',
          context: 'Simple addition function'
        },
        expect: result => result.messages && result.messages.length > 0
      }
    }
  });
}
```

### Pattern 3: Production Readiness Testing
**Before deploying MCP servers to production**

```javascript
// production-readiness.js
import { mcpTest, formatResults } from 'mcp-jest';

export async function productionReadinessTest(serverConfig) {
  console.log('🔍 Running production readiness tests...\n');
  
  const testSuites = [
    // Performance test
    {
      name: 'Performance Test',
      config: {
        tools: {
          search: { 
            args: { query: 'performance test' },
            expect: result => result.responseTime < 1000  // Must be under 1s
          }
        },
        timeout: 5000
      }
    },
    
    // Load test (multiple concurrent requests)
    {
      name: 'Load Test', 
      config: {
        tools: {
          search: { args: { query: 'load test' } },
          process: { args: { data: [1,2,3,4,5] } }
        },
        timeout: 10000
      }
    },
    
    // Error handling test
    {
      name: 'Error Handling',
      config: {
        tools: {
          invalidTool: { shouldThrow: true },
          searchEmpty: { 
            args: { query: '' },
            expect: result => result.error || result.results.length === 0
          }
        }
      }
    }
  ];
  
  const results = [];
  
  for (const suite of testSuites) {
    console.log(`Running ${suite.name}...`);
    const result = await mcpTest(serverConfig, suite.config);
    results.push({ name: suite.name, result });
    
    if (result.failed > 0) {
      console.log(`❌ ${suite.name} failed`);
      console.log(formatResults(result));
    } else {
      console.log(`✅ ${suite.name} passed (${result.duration}ms)`);
    }
  }
  
  const totalFailed = results.reduce((sum, r) => sum + r.result.failed, 0);
  
  if (totalFailed === 0) {
    console.log('\n🎉 Production readiness: PASSED');
    console.log('Your MCP server is ready for production deployment!');
  } else {
    console.log(`\n❌ Production readiness: FAILED (${totalFailed} failures)`);
    console.log('Please fix the issues before deploying to production.');
  }
  
  return totalFailed === 0;
}
```

### Pattern 4: Regression Testing 
**Ensure new changes don't break existing functionality**

```javascript
// regression-test.js
import { mcpTest } from 'mcp-jest';
import fs from 'fs/promises';

export async function regressionTest(serverConfig, baselineFile = 'mcp-baseline.json') {
  // Load baseline results
  let baseline = null;
  try {
    const baselineData = await fs.readFile(baselineFile, 'utf8');
    baseline = JSON.parse(baselineData);
  } catch (error) {
    console.log('No baseline found, creating new baseline...');
  }
  
  const currentResults = await mcpTest(serverConfig, {
    tools: {
      search: { args: { query: 'regression test' } },
      calculate: { args: { a: 5, b: 3 } },
      healthCheck: { expect: 'status === "healthy"' }
    },
    resources: ['config.json', 'data/*'],
    timeout: 30000
  });
  
  if (!baseline) {
    // Save current results as baseline
    await fs.writeFile(baselineFile, JSON.stringify({
      passed: currentResults.passed,
      total: currentResults.total,
      duration: currentResults.duration,
      testNames: currentResults.results.map(r => r.name)
    }, null, 2));
    
    console.log('✅ Baseline saved. Run again to compare against baseline.');
    return true;
  }
  
  // Compare with baseline
  const regressions = [];
  
  if (currentResults.passed < baseline.passed) {
    regressions.push(`Tests passing decreased: ${baseline.passed} → ${currentResults.passed}`);
  }
  
  if (currentResults.total !== baseline.total) {
    regressions.push(`Total tests changed: ${baseline.total} → ${currentResults.total}`);
  }
  
  const performanceRegression = currentResults.duration > (baseline.duration * 1.5);
  if (performanceRegression) {
    regressions.push(`Performance degraded: ${baseline.duration}ms → ${currentResults.duration}ms`);
  }
  
  if (regressions.length > 0) {
    console.log('❌ Regressions detected:');
    regressions.forEach(r => console.log(`   - ${r}`));
    return false;
  } else {
    console.log('✅ No regressions detected');
    return true;
  }
}
```

## Advanced Validation Patterns

### Built-in Expectation Helpers
```javascript
import { expect } from 'mcp-jest';

// Use these in your expect functions
const validationExamples = {
  // Basic existence and type checking
  exists: expect.exists(value),                    // value !== null && undefined
  notEmpty: expect.notEmpty(value),                // Non-empty arrays, strings, objects
  length: expect.length(value, 5),                 // Exact length match
  contains: expect.contains(value, 'substring'),   // String/array contains
  matches: expect.matches(value, /pattern/),       // Regex match
  
  // Combined validation patterns
  validUser: (result) => {
    return expect.exists(result.user) &&
           expect.notEmpty(result.user.email) &&
           expect.matches(result.user.email, /@/) &&
           expect.length(result.user.id, 36);  // UUID length
  },
  
  validSearchResults: (result) => {
    return expect.exists(result.results) &&
           Array.isArray(result.results) &&
           result.results.every(item => 
             expect.exists(item.title) && expect.exists(item.url)
           );
  }
};
```

### Custom Validation Functions
```javascript
// Create reusable validation helpers
export const customValidators = {
  // API response validator
  apiResponse: (expectedStatus = 200) => (result) => {
    return result.status === expectedStatus &&
           result.data !== undefined &&
           result.responseTime < 5000;
  },
  
  // Database result validator
  dbResult: (minRecords = 1) => (result) => {
    return Array.isArray(result.records) &&
           result.records.length >= minRecords &&
           result.records.every(record => record.id);
  },
  
  // File operation validator
  fileOperation: (operation) => (result) => {
    const validOps = ['read', 'write', 'delete', 'create'];
    return validOps.includes(operation) &&
           result.operation === operation &&
           result.success === true &&
           result.path;
  },
  
  // Performance validator
  performance: (maxTime = 1000) => (result) => {
    return result.executionTime <= maxTime &&
           result.memoryUsage < 100 * 1024 * 1024; // 100MB
  }
};

// Usage in tests
const results = await mcpTest(serverConfig, {
  tools: {
    getUsers: {
      args: { limit: 10 },
      expect: customValidators.dbResult(5)  // Expect at least 5 records
    },
    uploadFile: {
      args: { file: 'test.txt', content: 'test' },
      expect: customValidators.fileOperation('create')
    },
    searchFast: {
      args: { query: 'speed test' },
      expect: customValidators.performance(500)  // Must complete in 500ms
    }
  }
});
```

## Troubleshooting Guide for LLMs

### Common Issues and Solutions

#### 1. Server Startup Problems
```javascript
// Debug server startup issues
async function debugServerStartup(serverConfig) {
  console.log('🔧 Debugging server startup...');
  
  try {
    // Test with longer timeout and debug info
    const results = await mcpTest(
      {
        ...serverConfig,
        env: { ...serverConfig.env, DEBUG: 'mcp:*' }  // Enable debug logs
      },
      { 
        tools: [],  // No tools test, just connection
        timeout: 60000  // Long timeout
      }
    );
    
    console.log('Server started successfully!');
    return true;
    
  } catch (error) {
    if (error.message.includes('ENOENT')) {
      console.log('❌ Command not found. Check:');
      console.log('   - Is the command in PATH?');
      console.log('   - Use absolute path: /usr/bin/node');
      console.log('   - Check file permissions');
    } else if (error.message.includes('timeout')) {
      console.log('❌ Server startup timeout. Check:');
      console.log('   - Server logs for errors');
      console.log('   - Increase timeout value');
      console.log('   - Verify server starts manually');
    } else {
      console.log(`❌ Unknown error: ${error.message}`);
    }
    
    return false;
  }
}
```

#### 2. Tool Validation Failures
```javascript
// Debug tool execution issues
async function debugToolExecution(serverConfig, toolName, args = {}) {
  console.log(`🔧 Debugging tool: ${toolName}`);
  
  const results = await mcpTest(serverConfig, {
    tools: {
      [toolName]: {
        args,
        expect: (result) => {
          // Log the actual result structure
          console.log('Tool result structure:');
          console.log(JSON.stringify(result, null, 2));
          
          // Return true to pass the test and see the structure
          return true;
        }
      }
    }
  });
  
  return results;
}

// Usage
// await debugToolExecution({ command: 'node', args: ['./server.js'] }, 'search', { query: 'test' });
```

#### 3. Performance Issues
```javascript
// Diagnose performance problems
async function performanceDiagnosis(serverConfig) {
  const metrics = [];
  
  // Test individual operations
  const operations = [
    { name: 'Connection', config: { tools: [], timeout: 5000 } },
    { name: 'Tool Discovery', config: { tools: ['search'], timeout: 5000 } },
    { name: 'Tool Execution', config: { 
      tools: { search: { args: { query: 'test' } } }, 
      timeout: 5000 
    }}
  ];
  
  for (const op of operations) {
    const start = Date.now();
    const result = await mcpTest(serverConfig, op.config);
    const duration = Date.now() - start;
    
    metrics.push({
      operation: op.name,
      duration,
      success: result.failed === 0
    });
    
    console.log(`${op.name}: ${duration}ms (${result.failed === 0 ? 'PASS' : 'FAIL'})`);
  }
  
  // Identify bottlenecks
  const slowOps = metrics.filter(m => m.duration > 3000);
  if (slowOps.length > 0) {
    console.log('\n⚠️ Slow operations detected:');
    slowOps.forEach(op => {
      console.log(`   - ${op.operation}: ${op.duration}ms`);
    });
  }
  
  return metrics;
}
```

### Environment-Specific Debugging

```javascript
// Debug different environments
async function debugEnvironment(serverConfig, env = 'development') {
  console.log(`🔧 Debugging ${env} environment...`);
  
  const envConfigs = {
    development: {
      ...serverConfig,
      env: { 
        ...serverConfig.env, 
        NODE_ENV: 'development',
        DEBUG: 'mcp:*',
        LOG_LEVEL: 'debug'
      }
    },
    
    production: {
      ...serverConfig,
      env: { 
        ...serverConfig.env, 
        NODE_ENV: 'production',
        LOG_LEVEL: 'error'
      }
    },
    
    test: {
      ...serverConfig,
      env: { 
        ...serverConfig.env, 
        NODE_ENV: 'test',
        LOG_LEVEL: 'warn'
      }
    }
  };
  
  const config = envConfigs[env] || serverConfig;
  
  try {
    const results = await mcpTest(config, {
      tools: ['search'],  // Basic tool test
      timeout: 30000
    });
    
    console.log(`✅ ${env} environment working correctly`);
    return results;
    
  } catch (error) {
    console.log(`❌ ${env} environment issues:`);
    console.log(`   Error: ${error.message}`);
    
    // Environment-specific suggestions
    if (env === 'production') {
      console.log('   - Check production dependencies are installed');
      console.log('   - Verify production environment variables');
      console.log('   - Ensure production build is complete');
    } else if (env === 'development') {
      console.log('   - Check development dependencies');
      console.log('   - Verify development server configuration');
      console.log('   - Check for missing environment variables');
    }
    
    throw error;
  }
}
```

## Requirements and Compatibility

### System Requirements
- **Node.js**: 18+ (Required for ESM support and modern features)
- **Memory**: Minimum 256MB available RAM for testing
- **Disk**: Minimal disk space (mcp-jest is under 10MB)

### MCP Server Requirements  
- Must implement [Model Context Protocol](https://modelcontextprotocol.io) specification
- Must support stdio or stdio-compatible communication
- Should respond to capability discovery requests
- Should implement proper error handling

### Platform Compatibility
- ✅ **Linux**: Full support (Ubuntu, CentOS, Alpine, etc.)
- ✅ **macOS**: Full support (Intel and Apple Silicon)
- ✅ **Windows**: Full support (with proper Node.js installation)
- ✅ **Docker**: Excellent support for containerized testing
- ✅ **CI/CD**: Works with all major platforms (GitHub Actions, GitLab CI, Jenkins, etc.)

### Language Support for MCP Servers
- ✅ **Node.js/JavaScript**: Native support
- ✅ **TypeScript**: Native support with type definitions
- ✅ **Python**: Full support via python command
- ✅ **Go**: Support via compiled binaries
- ✅ **Rust**: Support via compiled binaries
- ✅ **C/C++**: Support via compiled binaries
- ✅ **Any Language**: As long as it implements MCP over stdio

## Key Features Summary

### For LLMs - Why This Matters
- **🔥 Zero Configuration**: Works out of the box with any MCP server
- **⚡ Fast Feedback**: Get test results in seconds, not minutes
- **🎯 Comprehensive Testing**: Tests connection, capabilities, and functionality
- **🔧 Debug-Friendly**: Detailed error messages and logging
- **🚀 CI/CD Ready**: Perfect for automated testing pipelines
- **📊 Performance Monitoring**: Built-in timing and performance metrics
- **🛡️ Error Handling**: Graceful handling of server failures and timeouts
- **📝 Clear Results**: Human-readable output with detailed diagnostics

### Core Capabilities
- **Connection Testing**: Verifies server starts and MCP handshake works
- **Capability Discovery**: Tests that tools, resources, and prompts are properly registered
- **Functional Testing**: Executes tools with arguments and validates results  
- **Error Testing**: Validates graceful handling of invalid inputs and edge cases
- **Performance Testing**: Measures response times and identifies bottlenecks
- **Regression Testing**: Compares current behavior against established baselines

## Repository and Community

- **📦 NPM Package**: https://www.npmjs.com/package/mcp-jest
- **📖 GitHub Repository**: https://github.com/josharsh/mcp-jest
- **📚 Documentation**: Available in the `/docs` folder
- **🐛 Issue Tracking**: GitHub Issues for bugs and feature requests
- **💬 Discussions**: GitHub Discussions for community support
- **📄 License**: MIT (free for commercial and open source use)

---

## Getting Started Checklist for LLMs

1. **✅ Install mcp-jest**: `npm install mcp-jest` or `npm install -g mcp-jest`
2. **✅ Create a basic test**: Copy the Quick Start example and adapt it
3. **✅ Test your MCP server**: Run `mcp-jest node ./your-server.js --tools your-tools`
4. **✅ Add to your workflow**: Integrate into package.json scripts or CI/CD
5. **✅ Expand testing**: Add expectations and validation for better coverage
6. **✅ Monitor in production**: Set up automated health checks

**You're now ready to build reliable, well-tested MCP servers that work seamlessly with AI assistants!**

This comprehensive guide provides everything LLMs need to successfully integrate mcp-jest into their MCP server development and testing workflows. The framework ensures reliable, production-ready MCP servers that provide robust interfaces between AI systems and external tools.